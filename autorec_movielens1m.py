# -*- coding: utf-8 -*-
"""AutoRec_MovieLens1M.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xjCUmwRSt0OGD_BeO1esDS-y_pSISnRx
"""

!pip install torchmetrics
import abc
import os
from zipfile import ZipFile
from pathlib import Path
import numpy as np
from scipy.sparse import coo_matrix
from scipy.sparse import csr_matrix
from sklearn.model_selection import train_test_split
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
from collections import defaultdict
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torchmetrics
from torchmetrics import Accuracy
from torchmetrics import Precision
from torchmetrics import Recall
from torchmetrics.classification import BinaryPrecisionRecallCurve
from sklearn import model_selection, metrics, preprocessing
from sklearn.metrics import accuracy_score
from sklearn.metrics import r2_score
from sklearn.metrics import confusion_matrix
from sklearn.datasets import make_circles
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
import os
from sklearn.utils.multiclass import unique_labels
import warnings
from zipfile import ZipFile as zip
from keras.models import Sequential
from keras.layers import Dense
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import LearningRateScheduler
from keras.optimizers import SGD
from keras.utils.np_utils import to_categorical
import tensorflow as tf
from keras.datasets import cifar10
import argparse

import requests
requests.packages.urllib3.disable_warnings()
import ssl
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    # Python heredado que no verifica los certificados HTTPS por defecto
    pass
else:
    # Manejar el entorno de destino que no soporta la verificación HTTPS
    ssl._create_default_https_context = _create_unverified_https_context

pd.set_option('display.max_columns', None)

# Descargamos la data actual desde http://files.grouplens.org/datasets/movielens/ml-1m.zip

movielens_data_file_url = \
  "https://files.grouplens.org/datasets/movielens/ml-1m.zip"


movielens_zipped_file = tf.keras.utils.get_file("ml-1m.zip",
                                                movielens_data_file_url,
                                                extract=False)

keras_datasets_path = Path(movielens_zipped_file).parents[0]
movielens_dir = keras_datasets_path / "ml-1m"

# Sólo se extraen los datos la primera vez que se ejecuta el script.
if not movielens_dir.exists():
    with ZipFile(movielens_zipped_file, "r") as zip:
        # Extract files
        print("Extrayendo todos los archivos ahora...")
        zip.extractall(path=keras_datasets_path)
        print("Descarga realizada con éxito!")

# Commented out IPython magic to ensure Python compatibility.
# Leemos los datos
columns = ['user_id', 'item_id', 'rating', 'timestamp']
movielens_file = os.path.join(movielens_dir, "ratings.dat")
df = pd.read_csv(movielens_file, sep='::', names=columns)
print('Hay %s usuarios, %s peliculas y %s observaciones en el dataset' \
#       %(df.user_id.unique().shape[0], df.item_id.unique().shape[0], df.shape[0]))
df.head()

items_id_file = os.path.join(movielens_dir, "movies.dat")
!head $items_id_file

columns = ['item_id', 'movie_title', 'Genres']
df_item_info = pd.read_csv(items_id_file, sep='::', names=columns, encoding = "latin")
df_item_info = df_item_info[['item_id', 'movie_title']].drop_duplicates()
df_item_info.head()

# comprobar datos únicos

len(df_item_info), len(df_item_info.item_id.unique()), len(df_item_info.movie_title.unique())

# Obtenemos las películas más valoradas
mostRated = df.groupby('item_id')['user_id'].count().reset_index()
mostRated.head()

mostRated = pd.merge(mostRated, df_item_info, on=['item_id'], how='inner').sort_values(by='user_id', ascending=False)
mostRated.head(10)

"""# **DIVISION DATA (ENTRENAMIENTO Y PRUEBAS)**"""

# Commented out IPython magic to ensure Python compatibility.
# PREPARAMOS LOS DATOS EN ENTRENAMIENTO Y PRUEBA PARA SU ANALISIS
# datos de entrenamiento (20%)
train, test = train_test_split(df, test_size=0.20)

# users, items and pares
df_users = train.drop_duplicates(subset=['user_id'])[['user_id']]
df_users['user'] = np.arange(len(df_users))
df_items = train.drop_duplicates(subset=['item_id'])[['item_id']]
df_items['item'] = np.arange(len(df_items))
num_users = len(df_users)
num_items = len(df_items)
num_pairs = train.shape[0]
print('Hay %s usuarios, %s peliculas y %s observaciones en los datos de entrenamiento' \
#       %(num_users, num_items, num_pairs))

# nuevos Ids renombrados
train = pd.merge(train, df_users, on=['user_id'])
train = pd.merge(train, df_items, on=['item_id'])

# get x, y values
x_train, y_train = train[['user', 'item']], train[['rating']].astype(float)

# Commented out IPython magic to ensure Python compatibility.
# datos de prueba - validación
test = pd.merge(test, df_users, on=['user_id'], how='inner')
test = pd.merge(test, df_items, on=['item_id'], how='inner')
print('Hay %s usuarios, %s peliculas y %s observaciones en los datos de prueba' \
#       %(len(test['user_id'].unique()), len(test['item_id'].unique()), test.shape[0]))

# get x, y values
x_test, y_test = test[['user', 'item']], test[['rating']].astype(float)

"""# **FACTORIZACION MATRICIAL**"""

class MatrixFactorization(tf.keras.Model):
    def __init__(self, num_users, num_items, embedding_size, **kwargs):
        super(MatrixFactorization, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_items = num_items
        self.embedding_size = embedding_size
        self.user_embedding = tf.keras.layers.Embedding(
            self.num_users,
            embedding_size,
            #embeddings_initializer="he_normal",
            embeddings_regularizer=tf.keras.regularizers.l2(1),
        )
        self.user_bias = tf.keras.layers.Embedding(self.num_users, 1)
        self.item_embedding = tf.keras.layers.Embedding(
            self.num_items,
            embedding_size,
            #embeddings_initializer="he_normal",
            embeddings_regularizer=tf.keras.regularizers.l2(1),
        )
        self.item_bias = tf.keras.layers.Embedding(self.num_items, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        item_vector = self.item_embedding(inputs[:, 1])
        item_bias = self.item_bias(inputs[:, 1])
        dot_user_item = tf.tensordot(user_vector, item_vector, 2)
        # Añade todos los componentes (incluido el sesgo)
        output = dot_user_item + user_bias + item_bias
        return output

# Params

EMBEDDING_SIZE = 20

model = MatrixFactorization(num_users, num_items, EMBEDDING_SIZE)
model.compile(loss=tf.keras.losses.MeanSquaredError(),
              optimizer=tf.keras.optimizers.Adam(lr=1e-3))

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=16, # rendimiento para la GPU
    epochs=10,
    verbose=1,
    validation_data=(x_test, y_test)
)

# Plot pérdida de entrenamiento

plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"], '--')
plt.title("model loss")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.grid()
plt.show()

def recommend(user_id, train, test, df_items, df_aux):
  movies_watched_by_user = train[train.user == user_id]

  df_items_aux = df_items[['item']]
  df_items_aux['user'] = user_id
  movies_not_watched = pd.merge(df_items_aux, movies_watched_by_user, on=['user', 'item'], how='left')
  movies_not_watched = movies_not_watched[movies_not_watched.rating.isnull()][['user', 'item']]

  ratings = model.predict(movies_not_watched).flatten()
  movies_not_watched['predicted_rating'] = ratings
  top_ratings_items = movies_not_watched.sort_values(by='predicted_rating', ascending=False)

  print("Mostrar recomendaciones para el usuario: {}".format(user_id))
  print("====" * 9)
  print("Películas con altas valoraciones de los usuarios")
  print("----" * 8)

  movies_watched_by_user = pd.merge(movies_watched_by_user, df_aux, on=['item_id'])
  top_movies_user = movies_watched_by_user.sort_values(by='rating', ascending=False)[['user', 'item', 'rating', 'movie_title']]
  print(top_movies_user.head(20))

  print("====" * 9)
  print("Calificaciones de las películas vistas en los datos de prueba")
  print("----" * 8)
  movies_watched_by_user_test = test[test.user == user_id]
  movies_watched_by_user_test = pd.merge(movies_watched_by_user_test, df_aux, on=['item_id'])
  movies_watched_by_user_test = pd.merge(movies_watched_by_user_test, movies_not_watched, on=['user', 'item'])
  movies_user_test = movies_watched_by_user_test.sort_values(by='rating', ascending=False)[['user', 'item', 'rating', 'predicted_rating', 'movie_title']]
  print(movies_user_test.head(20))

  print("----" * 8)
  print("Top 10 recomendaciones de películas")
  print("----" * 8)
  top_movies_recommended = pd.merge(top_ratings_items, df_items, on=['item'])
  top_movies_recommended = pd.merge(top_movies_recommended, df_aux, on=['item_id'])
  print(top_movies_recommended[['item', 'predicted_rating', 'movie_title']].head(20))

# Mostrar las 10 mejores recomendaciones de películas a un usuario

user_id = test.user.sample(1).iloc[0]
recommend(user_id, train, test, df_items, df_item_info)

"""# **AUTOENCODER**"""

# ****************---------------******************-----------------------#
# ****************---------------******************-----------------------#
# MODELO DE FC DE DNN BASADO EN AUTOENCODERS (AutoRec)
class AutoRec(tf.keras.Model):
  def __init__(self, num_hidden, num_users, dropout=0.06, reg_enc=1e-6, reg_dec=1e-6):
    super(AutoRec, self).__init__()
    self.encoder = tf.keras.layers.Dense(num_hidden, activation='sigmoid',
                                         use_bias=True,
                                         kernel_regularizer=tf.keras.regularizers.l2(reg_enc))
    self.decoder = tf.keras.layers.Dense(num_users, use_bias=True,
                                         kernel_regularizer=tf.keras.regularizers.l2(reg_dec))
    self.dropout = tf.keras.layers.Dropout(dropout)

  def call(self, input, training=None):
    hidden = self.dropout(self.encoder(input))
    pred = self.decoder(hidden)
    return pred

# Sistema métrico RMSE (FUNCION DE PERDIDA)

def autorec_rmse(y_true, y_pred):
  y_mask = tf.math.sign(y_true)
  squared_difference = tf.square(y_true - y_pred * y_mask)
  return tf.sqrt(tf.reduce_sum(squared_difference, axis=-1) / tf.math.maximum(tf.reduce_sum(y_mask, axis=-1), 1))

# data para entrenamiento

ae_data_train = train.groupby('user').agg({'item': list, 'rating': list}).reset_index()
ae_data_train['input'] = ae_data_train.apply(lambda x: np.squeeze(coo_matrix((x[2], (np.zeros_like(x[1]), x[1])),
                                                                  shape=(1, num_items)).toarray()),
                                             axis=1)
ae_data_train.head()

x_train_autorec = ae_data_train.values[:, 3]
x_train_autorec = np.stack(x_train_autorec).astype(float)
x_train_autorec

x_train_autorec.shape

# data para test-validación
ae_data_test = test.groupby('user').agg({'item': list, 'rating': list}).reset_index()
ae_data_test['input'] = ae_data_test.apply(lambda x: np.squeeze(coo_matrix((x[2], (np.zeros_like(x[1]), x[1])),
                                                                shape=(1, num_items)).toarray()),
                                           axis=1)

ae_data_test_aux = pd.merge(ae_data_train, ae_data_test, on=['user'], how='inner')
x_test_autorec = np.stack(ae_data_test_aux.values[:, -1]).astype(float)
x_test_autorec

x_test_autorec.shape

# parámetros VARIAR VALORES
NUM_HIDDEN = 1200
DROPOUT = 0.06
REG_ENC = 1e-6
REG_DEC = 1e-6

model = AutoRec(NUM_HIDDEN, num_items, DROPOUT, REG_ENC, REG_DEC)
model.compile(loss=autorec_rmse,
              optimizer=tf.keras.optimizers.Adam(lr=1e-3))

history = model.fit(
    x_train_autorec,
    x_train_autorec,
    batch_size=550, # tamaño del entrenamiento por lotes
    epochs=2000,
    verbose=1,
    validation_data=(x_test_autorec, x_test_autorec))

# Plot pérdida de entrenamiento

plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"], '--')
plt.title("model loss")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

"""# **Evaluación Precision**"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf

from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from tensorflow.keras import layers, losses
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Model

reconstructions = model.predict(x_train_autorec)
train_loss = tf.keras.losses.mae(reconstructions, x_train_autorec)

plt.hist(train_loss[None,:], bins=50)
plt.xlabel("Train loss")
plt.ylabel("No of examples")
plt.show()

threshold = np.mean(train_loss) + np.std(train_loss)
print("Threshold: ", threshold)

reconstructions = model.predict(x_test_autorec)
test_loss = tf.keras.losses.mae(reconstructions, x_test_autorec)

plt.hist(test_loss[None, :], bins=50)
plt.xlabel("Test loss")
plt.ylabel("No of examples")
plt.show()

x_test_autorec

labels = x_test_autorec[:, -1]
labels

def predict(modelo, data, threshold):
  reconstructions = model(data)
  loss = tf.keras.losses.mae(reconstructions, data)
  return tf.math.less(loss, threshold)

def print_stats(predictions, labels):
  print("Accuracy = {}".format(accuracy_score(labels, predictions)))
  print("Precision = {}".format(precision_score(labels, predictions)))
  print("Recall = {}".format(recall_score(labels, predictions)))

preds = predict(model, x_test_autorec, threshold)
print_stats(preds, labels)

preds

-------------------------------------------------------

# evaluamos el modelo
scores = model.evaluate(x_train_autorec, x_test_autorec)

print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
print (model.predict(x_train_autorec).round())

valid_x_predictions = model.predict(x_test_autorec)
mse = np.mean(np.power(x_test_autorec - valid_x_predictions, 2), axis=1)

precision_rt, recall_rt, threshold_rt = 3.5
plt.plot(threshold_rt, precision_rt[1:], label="Precision",linewidth=5)
plt.plot(threshold_rt, recall_rt[1:], label="Recall",linewidth=5)
plt.title('Precision and recall for different threshold values')
plt.xlabel('Threshold')
plt.ylabel('Precision/Recall')
plt.legend()
plt.show()

# Predicción X_test -> model -> X_pred
X_pred = model.predict(x_test_autorec)
ecm = autorec_rmse(x_test_autorec, X_pred)
#ecm = np.mean(np.power(x_test_autorec-X_pred,2), axis=1)
print(X_pred.shape)

ecm

x_test_autorec

# Matriz de confusión
umbral_fijo = 0.70
Y_pred = [1 if e > umbral_fijo else 0 for e in ecm]
Y_pred

conf_matrix = confusion_matrix(x_test_autorec, Y_pred)
print(conf_matrix)

"""# **RECOMENDACION AUTOREC**"""

def recommend_autorec(user_id, train, test, df_items, df_aux, num_items):
  movies_watched_by_user = train[train.user == user_id]

  df_items_aux = df_items[['item']]
  df_items_aux['user'] = user_id
  movies_not_watched = pd.merge(df_items_aux, movies_watched_by_user, on=['user', 'item'], how='left')
  movies_not_watched = movies_not_watched[movies_not_watched.rating.isnull()][['user', 'item']]

  df_input = movies_watched_by_user.groupby('user').agg({'item': list, 'rating': list}).reset_index()
  df_input['input'] = df_input.apply(lambda x: np.squeeze(coo_matrix((x[2], (np.zeros_like(x[1]), x[1])),
                                                          shape=(1, num_items)).toarray()),
                                     axis=1)

  x = np.stack(df_input.values[:, -1]).astype(float)
  ratings = model.predict(x).flatten()
  ratings_items = pd.DataFrame({'item': np.arange(num_items), 'predicted_rating': ratings})
  ratings_items = pd.merge(ratings_items, movies_not_watched, on=['item'], how='left')
  top_ratings_items = ratings_items[ratings_items.user.notnull()] \
    .sort_values(by='predicted_rating', ascending=False)[['item', 'predicted_rating']]

  print("Mostrar recomendaciones para el usuario: {}".format(user_id))
  print("====" * 9)
  print("Películas mejor valoradas por los usuarios")
  print("----" * 8)

  movies_watched_by_user = pd.merge(movies_watched_by_user, df_aux, on=['item_id'])
  top_movies_user = movies_watched_by_user.sort_values(by='rating', ascending=False)[['user', 'item', 'rating', 'movie_title']]
  print(top_movies_user.head(20))

  print("====" * 9)
  print("Rating de las películas vistas en la validación")
  print("----" * 8)
  movies_watched_by_user_test = test[test.user == user_id]
  movies_watched_by_user_test = pd.merge(movies_watched_by_user_test, df_aux, on=['item_id'])
  movies_watched_by_user_test = pd.merge(movies_watched_by_user_test, ratings_items, on=['user', 'item'])
  movies_user_test = movies_watched_by_user_test.sort_values(by='rating', ascending=False)[['user', 'item', 'rating', 'predicted_rating', 'movie_title']]
  print(movies_user_test.head(20))

  print("----" * 8)
  print("Top 10 recomendación de películas")
  print("----" * 8)
  top_movies_recommended = pd.merge(top_ratings_items, df_items, on=['item'])
  top_movies_recommended = pd.merge(top_movies_recommended, df_aux, on=['item_id'])
  print(top_movies_recommended[['item', 'predicted_rating', 'movie_title']].head(10))

#Mostrar las 10 mejores recomendaciones de películas a un usuario

user_id = test.user.sample(1).iloc[0]
recommend_autorec(user_id, train, test, df_items, df_item_info, num_items)